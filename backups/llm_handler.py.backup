"""
Borgo-Bot v3.5 - LLM Handler
Phase 4: Multi-Model-Fallback und Response-Validierung
"""

import re
import asyncio
import logging
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import aiohttp

from config import (
    LLM_MODELS,
    PRIMARY_MODEL,
    MAX_LLM_RETRIES,
    LLM_TIMEOUT_SECONDS,
    HALLUCINATION_PATTERNS,
    CONTEXT_MIXING_RULES,
    MIN_RESPONSE_LENGTH,
    MAX_RESPONSE_LENGTH,
    QUALITY_CHECKS
)

logger = logging.getLogger(__name__)


class LLMHandler:
    """
    Verwaltet LLM-Anfragen mit Fallback und Validierung
    Erkennt Halluzinationen und Context-Mixing
    """
    
    def __init__(self, ollama_url: str = "http://localhost:11434"):
        self.ollama_url = ollama_url
        # Models als Instance-Variable (k√∂nnen von au√üen √ºberschrieben werden)
        self.models = LLM_MODELS
        self.primary_model = PRIMARY_MODEL
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'retries_used': 0,
            'hallucinations_detected': 0,
            'context_mixing_detected': 0,
            'model_usage': {model: 0 for model in self.models},
        }
    
    async def generate_response(
        self,
        query: str,
        context: str,
        max_retries: int = MAX_LLM_RETRIES
    ) -> Tuple[Optional[str], Dict]:
        """
        Generiert LLM-Response mit Fallback und Validierung
        
        Args:
            query: User-Query
            context: Vorbereiteter Context
            max_retries: Max Retry-Versuche
        
        Returns:
            (response, metadata)
        """
        self.stats['total_requests'] += 1
        
        start_time = datetime.now()
        metadata = {
            'attempts': [],
            'final_model': None,
            'validation_issues': [],
            'processing_time_ms': 0,
        }
        
        # Versuche Modelle der Reihe nach
        for attempt, model in enumerate(self.models[:max_retries + 1]):
            try:
                logger.info(f"ü§ñ Attempt {attempt + 1}: Using model '{model}'")
                
                # LLM-Call
                response = await self._call_ollama(query, context, model)
                
                # Validierung
                is_valid, issues = self._validate_response(response, query)
                
                attempt_data = {
                    'model': model,
                    'success': is_valid,
                    'issues': issues,
                    'response_length': len(response) if response else 0,
                }
                metadata['attempts'].append(attempt_data)
                
                if is_valid:
                    # Erfolg!
                    self.stats['successful_requests'] += 1
                    self.stats['model_usage'][model] = self.stats['model_usage'].get(model, 0) + 1
                    metadata['final_model'] = model
                    
                    duration = (datetime.now() - start_time).total_seconds() * 1000
                    metadata['processing_time_ms'] = round(duration, 2)
                    
                    logger.info(f"‚úÖ Valid response from '{model}' ({duration:.0f}ms)")
                    return response, metadata
                else:
                    # Validierung fehlgeschlagen
                    logger.warning(f"‚ùå Invalid response from '{model}': {issues}")
                    self.stats['retries_used'] += 1
                    
                    # Z√§hle spezifische Issues
                    for issue in issues:
                        if 'hallucination' in issue.lower():
                            self.stats['hallucinations_detected'] += 1
                        if 'context_mixing' in issue.lower():
                            self.stats['context_mixing_detected'] += 1
            
            except Exception as e:
                logger.error(f"‚ùå Model '{model}' failed: {e}", exc_info=True)
                metadata['attempts'].append({
                    'model': model,
                    'success': False,
                    'error': str(e),
                })
                continue
        
        # Alle Modelle gescheitert
        self.stats['failed_requests'] += 1
        metadata['validation_issues'] = ['All models failed or produced invalid responses']
        
        duration = (datetime.now() - start_time).total_seconds() * 1000
        metadata['processing_time_ms'] = round(duration, 2)
        
        logger.error(f"‚ùå All models failed after {len(metadata['attempts'])} attempts")
        return None, metadata
    
    async def _call_ollama(
        self,
        query: str,
        context: str,
        model: str
    ) -> str:
        """
        Ruft Ollama API auf
        
        Returns:
            LLM-Response als String
        """
        prompt = self._build_prompt(query, context, model)
        
        payload = {
            'model': model,
            'prompt': prompt,
            'stream': False,
            'options': {
                'temperature': 0.3,  # Niedrig f√ºr pr√§zise Antworten
                'top_p': 0.9,
                'top_k': 40,
            }
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.ollama_url}/api/generate",
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=LLM_TIMEOUT_SECONDS)
                ) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        response = data.get('response', '').strip()
                        
                        logger.debug(f"LLM response length: {len(response)} chars")
                        return response
                    else:
                        error_text = await resp.text()
                        raise Exception(f"Ollama API error {resp.status}: {error_text}")
        
        except asyncio.TimeoutError:
            raise Exception(f"Timeout after {LLM_TIMEOUT_SECONDS}s")
        except Exception as e:
            raise Exception(f"Ollama call failed: {e}")
    
    def _build_prompt(self, query: str, context: str, model: str = None) -> str:
        """Baut LLM-Prompt aus Query und Context"""
        
        # F√ºr qwen-Modelle: Explizit Deutsch verlangen!
        language_instruction = ""
        if model and 'qwen' in model.lower():
            language_instruction = "WICHTIG: Antworte ausschlie√ülich auf Deutsch!\n\n"
        
        prompt_parts = [
            language_instruction,
            # REGELN ZUERST (als Meta-Instruktion)
            "Du bist Borgi, der hilfreiche Borgo Batone G√§ste-Assistent.",
            "",
            "ANWEISUNGEN (befolge diese, aber gib sie NICHT in deiner Antwort wieder):",
            "‚Ä¢ Nutze NUR Informationen aus der Knowledge Base unten",
            "‚Ä¢ Gib Informationen pr√§zise wieder - nutze die Original-Formulierungen",
            "‚Ä¢ √úbernimm Listen und Nummerierungen wie vorgegeben",
            "‚Ä¢ Wenn du etwas nicht wei√üt, sage es ehrlich",
            "‚Ä¢ Erfinde KEINE Details, Zahlen oder Einheiten",
            "",
            "---",
            "",
            "# KNOWLEDGE BASE",
            "",
            context,
            "",
            "---",
            "",
            "# FRAGE",
            query,
            "",
            "# ANTWORT",
            "(Gib NUR die relevanten Informationen aus der Knowledge Base, NICHT die Anweisungen oben)",
            "",
        ]
        
        # Entferne leere Strings am Anfang wenn language_instruction leer ist
        if not language_instruction:
            prompt_parts = prompt_parts[1:]
        
        return "\n".join(prompt_parts)
    
    def _validate_response(
        self,
        response: str,
        query: str
    ) -> Tuple[bool, List[str]]:
        """
        Validiert LLM-Response auf Qualit√§t
        
        Returns:
            (is_valid, list_of_issues)
        """
        issues = []
        
        if not response:
            issues.append("Empty response")
            return False, issues
        
        # Check 1: L√§nge
        if QUALITY_CHECKS.get('too_short'):
            if len(response) < MIN_RESPONSE_LENGTH:
                issues.append(f"Too short ({len(response)} chars)")
        
        if QUALITY_CHECKS.get('too_long'):
            if len(response) > MAX_RESPONSE_LENGTH:
                issues.append(f"Too long ({len(response)} chars)")
        
        # Check 2: Halluzinationen
        if QUALITY_CHECKS.get('hallucination'):
            hallucination_found = self._check_hallucinations(response)
            if hallucination_found:
                issues.append(f"Hallucination detected: {hallucination_found}")
        
        # Check 3: Context-Mixing
        if QUALITY_CHECKS.get('context_mixing'):
            mixing_found = self._check_context_mixing(response, query)
            if mixing_found:
                issues.append(f"Context mixing: {mixing_found}")
        
        # Check 4: Unvollst√§ndige Antwort
        if QUALITY_CHECKS.get('incomplete'):
            if self._is_incomplete(response):
                issues.append("Incomplete response")
        
        # Check 5: System Prompt Leakage (NEU!)
        if self._has_prompt_leakage(response):
            issues.append("System prompt leaked in response")
        
        is_valid = len(issues) == 0
        
        return is_valid, issues
    
    def _has_prompt_leakage(self, response: str) -> bool:
        """
        Pr√ºft ob System-Instruktionen in Response durchgesickert sind
        
        Returns:
            True wenn Prompt-Leakage erkannt wurde
        """
        leakage_patterns = [
            r'KRITISCHE REGELN',
            r'ANWEISUNGEN.*befolge',
            r'gib.*NICHT.*in deiner Antwort wieder',
            r'Du bist.*Assistent.*befolge',
            r'KNOWLEDGE BASE',
            r'# FRAGE',
            r'# ANTWORT',
        ]
        
        for pattern in leakage_patterns:
            if re.search(pattern, response, re.IGNORECASE):
                logger.warning(f"‚ö†Ô∏è Prompt leakage detected: '{pattern}'")
                return True
        
        return False
    
    def _check_hallucinations(self, response: str) -> Optional[str]:
        """
        Pr√ºft auf bekannte Halluzinations-Muster
        
        Returns:
            Beschreibung der Halluzination oder None
        """
        for pattern, description in HALLUCINATION_PATTERNS:
            if re.search(pattern, response, re.IGNORECASE):
                logger.warning(f"Hallucination pattern found: {description}")
                return description
        
        return None
    
    def _check_context_mixing(
        self,
        response: str,
        query: str
    ) -> Optional[str]:
        """
        Pr√ºft auf Context-Mixing (z.B. Pizza + Rasenm√§her)
        
        Returns:
            Beschreibung des Mixings oder None
        """
        response_lower = response.lower()
        query_lower = query.lower()
        
        for topic, forbidden_words in CONTEXT_MIXING_RULES.items():
            # Wenn Query zu diesem Topic geh√∂rt
            if topic in query_lower:
                # Pr√ºfe ob forbidden words in Response sind
                for forbidden in forbidden_words:
                    if forbidden in response_lower:
                        mixing = f"Topic '{topic}' mixed with '{forbidden}'"
                        logger.warning(f"Context mixing detected: {mixing}")
                        return mixing
        
        return None
    
    def _is_incomplete(self, response: str) -> bool:
        """
        Pr√ºft ob Antwort unvollst√§ndig wirkt
        """
        incomplete_patterns = [
            r'\.\.\.$',  # Endet mit "..."
            r'\b(und|oder|bzw|etc)\s*$',  # Endet mit Bindewort
            r'\baber\s*$',  # Endet mit "aber"
        ]
        
        for pattern in incomplete_patterns:
            if re.search(pattern, response, re.IGNORECASE):
                return True
        
        return False
    
    def get_stats(self) -> Dict:
        """Gibt LLM-Handler Statistiken zur√ºck"""
        total = self.stats['total_requests']
        if total == 0:
            return self.stats
        
        return {
            **self.stats,
            'success_rate_percent': round(
                (self.stats['successful_requests'] / total) * 100, 2
            ),
            'failure_rate_percent': round(
                (self.stats['failed_requests'] / total) * 100, 2
            ),
            'avg_retries': round(
                self.stats['retries_used'] / total, 2
            ),
        }


class ResponseFormatter:
    """
    Formatiert und verbessert LLM-Responses
    Cleanup und Konsistenz
    """
    
    @staticmethod
    def format(response: str) -> str:
        """Formatiert Response f√ºr bessere Lesbarkeit"""
        
        # Entferne f√ºhrende/trailing Whitespace
        response = response.strip()
        
        # Normalisiere Leerzeichen
        response = re.sub(r'\s+', ' ', response)
        
        # Stelle sicher dass S√§tze mit Gro√übuchstaben starten
        response = ResponseFormatter._capitalize_sentences(response)
        
        # F√ºge Leerzeichen nach Satzzeichen hinzu falls fehlend
        response = re.sub(r'([.!?])([A-Z√Ñ√ñ√ú])', r'\1 \2', response)
        
        return response
    
    @staticmethod
    def _capitalize_sentences(text: str) -> str:
        """Kapitalisiert Satzanf√§nge"""
        sentences = re.split(r'([.!?]\s+)', text)
        
        result = []
        for i, part in enumerate(sentences):
            if i % 2 == 0 and part:  # Nur S√§tze, nicht Trenner
                # Ersten Buchstaben gro√üschreiben
                part = part[0].upper() + part[1:] if len(part) > 1 else part.upper()
            result.append(part)
        
        return ''.join(result)
    
    @staticmethod
    def add_emoji(response: str, sentiment: str = 'neutral') -> str:
        """F√ºgt passende Emojis hinzu (optional)"""
        emoji_map = {
            'positive': 'üòä',
            'helpful': 'üëç',
            'warning': '‚ö†Ô∏è',
            'safety': 'üö®',
            'question': '‚ùì',
        }
        
        if sentiment in emoji_map:
            return f"{response} {emoji_map[sentiment]}"
        
        return response


# ========================================
# TESTS
# ========================================

async def test_llm_handler():
    """Test-Suite f√ºr LLM Handler"""
    
    handler = LLMHandler()
    formatter = ResponseFormatter()
    
    # Mock-Context f√ºr Tests
    test_context = """
# BORGO BATONE KNOWLEDGE BASE

Du bist Borgi, der Borgo-Batone G√§ste-Assistent.

## PIZZA

F√ºr 24 Personen Pizza brauchst du 3 kg Mehl, 3 W√ºrfel Hefe, 
3 gro√üe Flaschen Passata und 1,5 kg Mozzarella.

# WICHTIGE REGELN
1. Antworte NUR mit Informationen aus obigen Eintr√§gen
2. Erfinde KEINE Zahlen oder Einheiten
    """
    
    test_queries = [
        "Wie viel Mehl f√ºr 24 Personen Pizza?",
        "Sind Hunde erlaubt?",  # Nicht im Context
    ]
    
    print("=" * 70)
    print("LLM HANDLER TESTS")
    print("=" * 70)
    
    for query in test_queries:
        print(f"\nüìù Query: '{query}'")
        
        try:
            # Generiere Response
            response, metadata = await handler.generate_response(
                query, test_context, max_retries=1
            )
            
            if response:
                # Formatiere
                formatted = formatter.format(response)
                
                print(f"\n‚úÖ Response ({len(formatted)} chars):")
                print(f"   {formatted[:200]}...")
                print(f"\n   Model: {metadata['final_model']}")
                print(f"   Time: {metadata['processing_time_ms']}ms")
                print(f"   Attempts: {len(metadata['attempts'])}")
            else:
                print(f"\n‚ùå Failed to generate response")
                print(f"   Issues: {metadata['validation_issues']}")
                for attempt in metadata['attempts']:
                    print(f"   Attempt: {attempt}")
        
        except Exception as e:
            print(f"\n‚ùå Error: {e}")
    
    # Statistiken
    print("\n" + "=" * 70)
    print("STATISTIKEN")
    print("=" * 70)
    stats = handler.get_stats()
    for key, value in stats.items():
        if key != 'model_usage':
            print(f"  {key}: {value}")
    
    print("\n  Model Usage:")
    for model, count in stats['model_usage'].items():
        print(f"    {model}: {count}")
    
    print("=" * 70)


if __name__ == "__main__":
    # Asyncio Event Loop f√ºr Tests
    asyncio.run(test_llm_handler())
